{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stacking classifier \n",
    "## using sklearn and mlxtend\n",
    "## fitted models are stored for predicting unseen test data\n",
    "\n",
    "## Author: Yichao Li\n",
    "## Last modified: 9-3-2017\n",
    "\n",
    "## Usage:\n",
    "## from stacking_clf import mixer_clf\n",
    "## model = mixer_clf(X,y)\n",
    "## feature selection method is not included\n",
    "## parameter tuning is done using random search\n",
    "\n",
    "## Note: class_weight={0:.3, 1:.7} is pre-assigned\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, auc, f1_score, matthews_corrcoef\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifierCV,SGDClassifier,SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor,RadiusNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier,MLPRegressor\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.gaussian_process import GaussianProcessClassifier,GaussianProcessRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso,ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge,LinearRegression,HuberRegressor,PassiveAggressiveRegressor,BayesianRidge,ARDRegression,Lars,LassoLars,LassoLarsIC,MultiTaskLasso,OrthogonalMatchingPursuit,RANSACRegressor,TheilSenRegressor\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "\n",
    "def mixer_reg(X,y,RANDOM_SEED = 42,n_iter=3):\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=RANDOM_SEED)\n",
    "\tparams={}\n",
    "\tridge = Ridge()\n",
    "\tparams['ridge__alpha'] = [0.5,1,5,10]\n",
    "\tlasso = Lasso(selection =\"random\",alpha=1.0)\n",
    "\tparams['lasso__alpha'] = [0.5,1,5,10]\n",
    "\trf = RandomForestRegressor(n_estimators=50,criterion=\"mae\",max_features='auto')\n",
    "\t# ENet = ElasticNet(alpha=1,l1_ratio=0.3)\n",
    "\t# params['elasticnet__alpha'] = [0.5,1,5,10]\n",
    "\t# params['elasticnet__l1_ratio'] = [0.3,0.5,0.8]\n",
    "\t# GBR = GradientBoostingRegressor(loss=\"ls\",max_depth=3)\n",
    "\t# params['gradientboostingregressor__loss'] = [\"ls\",\"lad\",\"huber\",\"quantile\"]\n",
    "\t# params['gradientboostingregressor__max_depth'] = [3,8,20]\n",
    "\t# ETR = ExtraTreesRegressor(n_estimators=50,criterion=\"mae\")\n",
    "\t# KNNR = KNeighborsRegressor(n_neighbors=20,p=2)\n",
    "\t# params['kneighborsregressor__n_neighbors'] = [5,20]\n",
    "\t# SVRR = SVR(kernel = \"linear\",C=1)\n",
    "\t# LR = LinearRegression()\n",
    "\t# HR = HuberRegressor()\n",
    "\t# params['huberregressor__epsilon'] = [1.1,1.35,3]\n",
    "\t# PAR1 = PassiveAggressiveRegressor(loss=\"epsilon_insensitive\")\n",
    "\t# PAR2 = PassiveAggressiveRegressor(loss=\"squared_epsilon_insensitive\")\n",
    "\t# BayesR = BayesianRidge()\n",
    "\t# ARDR = ARDRegression()\n",
    "\t# LARSR = Lars()\n",
    "\t# LassoLarsR = LassoLars()\n",
    "\t# LassoLarsICR = LassoLarsIC()\n",
    "\t# OrthogonalMatchingPursuitR = OrthogonalMatchingPursuit()\n",
    "\t# RANSACRegressorR = RANSACRegressor()\n",
    "\t# SGDR = SGDRegressor()\n",
    "\t# TheilSenRegressorR = TheilSenRegressor()\n",
    "\t# GaussianProcessRegressorR = GaussianProcessRegressor()\n",
    "\t# ada_ridge = AdaBoostRegressor(base_estimator=Ridge())\n",
    "\t# ada_KNN = AdaBoostRegressor(base_estimator=KNeighborsRegressor(n_neighbors=5))\n",
    "\t# ada_lasso = AdaBoostRegressor(base_estimator=Lasso())\n",
    "\t# ada_DT = AdaBoostRegressor()\n",
    "\t# ada_HR = AdaBoostRegressor(base_estimator=HuberRegressor(epsilon = 1.35,alpha =0.01))\n",
    "\t# ada_EN = AdaBoostRegressor(base_estimator=ElasticNet())\n",
    "\t# ada_LR = AdaBoostRegressor(base_estimator=LinearRegression())\n",
    "\txgbR = xgb.sklearn.XGBRegressor(n_estimators=500,nthread=1)\n",
    "# \tmy_reg_list = [ridge,lasso,rf,ENet,GBR,ETR,KNNR,SVRR,LR,HR,PAR1,PAR2,BayesR,ARDR,LARSR,LassoLarsR,LassoLarsICR,OrthogonalMatchingPursuitR,RANSACRegressorR,SGDR,TheilSenRegressorR,GaussianProcessRegressorR,ada_ridge,ada_KNN,ada_lasso,ada_DT,ada_HR,ada_EN,ada_LR,xgbR]\n",
    "\tmy_reg_list = [ridge,lasso,rf,xgbR]\n",
    "\tmix_CV_regressor = StackingCVRegressor(regressors=my_reg_list, meta_regressor=Lasso(),use_features_in_secondary = False,cv = 2)\t\n",
    "\tparams['meta-lasso__alpha'] = [0.01, 0.1 ,0.5, 1.0 ,5, 10.0]\n",
    "\t# params['meta-xgbregressor__max_depth'] = [1,3,5,7,10,15,50]\n",
    "\t# params['meta-xgbregressor__learning_rate'] = [0.1,0.01,1,10]\n",
    "\t# params['meta-xgbregressor__gamma'] = [0.1,0.01,0.5,0.8]\n",
    "\t# params['meta-xgbregressor__min_child_weight'] = [1,5,10,20]\n",
    "\t# params['meta-xgbregressor__max_delta_step'] = [1,5,10,20]\n",
    "\t# params['meta-xgbregressor__reg_alpha'] = [0.1,0.01,1,10]\n",
    "\t# params['meta-xgbregressor__reg_lambda'] = [0.1,0.01,1,10]\n",
    "\t\n",
    "\tgrid = RandomizedSearchCV(estimator=mix_CV_regressor, param_distributions=params, cv=2,n_iter=n_iter,refit=True,n_jobs=-1,scoring=\"neg_mean_absolute_error\",verbose=10)\t\t\t\t\t\t\n",
    "\tgrid.fit(X_train, y_train)\n",
    "\tprint('Best parameters: %s' % grid.best_params_)\n",
    "\tprint('CV best MAE: %.7f' % grid.best_score_)\n",
    "\tbest_clf = grid.best_estimator_\n",
    "\tpredict_value = best_clf.predict(X_test)\n",
    "\tMAE = mean_absolute_error(y_test,predict_value)\n",
    "\tMSE = mean_squared_error(y_test,predict_value)\n",
    "\tprint('testing MAE: %.7f' % MAE)\n",
    "\tprint('testing MSE: %.7f' % MSE)\n",
    "\treturn best_clf\n",
    "\t\n",
    "\n",
    "def mixer_reg_bk(X,y,RANDOM_SEED = 42,n_iter=3):\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=RANDOM_SEED)\n",
    "\tparams={}\n",
    "\tridge = Ridge()\n",
    "\tparams['ridge__alpha'] = [0.5,1,5,10]\n",
    "\t# ridge2 = Ridge(alpha=0.1)\n",
    "\t# ridge3 = Ridge(alpha=10)\n",
    "\tlasso = Lasso(selection =\"random\",alpha=1.0)\n",
    "\tparams['lasso__alpha'] = [0.5,1,5,10]\n",
    "\t# lasso2 = Lasso(selection =\"random\",alpha=0.1)\n",
    "\t# lasso3 = Lasso(selection =\"random\",alpha=10)\n",
    "\t# lasso4 = Lasso(selection =\"cyclic\",alpha=1.0)\n",
    "\t# lasso5 = Lasso(selection =\"cyclic\",alpha=0.1)\n",
    "\t# lasso6 = Lasso(selection =\"cyclic\",alpha=10)\n",
    "\t# rf1 = RandomForestRegressor(n_estimators=50,criterion=\"mse\",max_features=500)\n",
    "\t# rf2 = RandomForestRegressor(n_estimators=50,criterion=\"mse\",max_features=300)\n",
    "\t# rf3 = RandomForestRegressor(n_estimators=50,criterion=\"mse\",max_features=100)\n",
    "\t# rf4 = RandomForestRegressor(n_estimators=50,criterion=\"mse\",max_features='auto')\n",
    "\t# rf5 = RandomForestRegressor(n_estimators=50,criterion=\"mae\",max_features=500)\n",
    "\t# rf6 = RandomForestRegressor(n_estimators=50,criterion=\"mae\",max_features=300)\n",
    "\t# rf7 = RandomForestRegressor(n_estimators=50,criterion=\"mae\",max_features=100)\n",
    "\trf = RandomForestRegressor(n_estimators=50,criterion=\"mae\",max_features='auto')\n",
    "\t# ENet1 = ElasticNet(alpha=1,l1_ratio=0.3)\n",
    "\tENet = ElasticNet(alpha=1,l1_ratio=0.3)\n",
    "\tparams['elasticnet__alpha'] = [0.5,1,5,10]\n",
    "\tparams['elasticnet__l1_ratio'] = [0.3,0.5,0.8]\n",
    "\t# ENet2 = ElasticNet(alpha=1,l1_ratio=0.5)\n",
    "\t# ENet3 = ElasticNet(alpha=1,l1_ratio=0.8)\n",
    "\t# ENet4 = ElasticNet(alpha=10,l1_ratio=0.3)\n",
    "\t# ENet5 = ElasticNet(alpha=10,l1_ratio=0.5)\n",
    "\t# ENet6 = ElasticNet(alpha=10,l1_ratio=0.8)\n",
    "\t# ENet7 = ElasticNet(alpha=0.1,l1_ratio=0.3)\n",
    "\t# ENet8 = ElasticNet(alpha=0.1,l1_ratio=0.5)\n",
    "\t# ENet9 = ElasticNet(alpha=0.1,l1_ratio=0.8)\n",
    "\tGBR = GradientBoostingRegressor(loss=\"ls\",max_depth=3)\n",
    "\tparams['gradientboostingregressor__loss'] = [\"ls\",\"lad\",\"huber\",\"quantile\"]\n",
    "\tparams['gradientboostingregressor__max_depth'] = [3,8,20]\n",
    "\t# GBR1 = GradientBoostingRegressor(loss=\"ls\",max_depth=3)\n",
    "\t# GBR2 = GradientBoostingRegressor(loss=\"lad\",max_depth=3)\n",
    "\t# GBR3 = GradientBoostingRegressor(loss=\"huber\",max_depth=3)\n",
    "\t# GBR4 = GradientBoostingRegressor(loss=\"quantile\",max_depth=3)\n",
    "\t# GBR5 = GradientBoostingRegressor(loss=\"ls\",max_depth=6)\n",
    "\t# GBR6 = GradientBoostingRegressor(loss=\"lad\",max_depth=6)\n",
    "\t# GBR7 = GradientBoostingRegressor(loss=\"huber\",max_depth=6)\n",
    "\t# GBR8 = GradientBoostingRegressor(loss=\"quantile\",max_depth=6)\n",
    "\t# GBR9 = GradientBoostingRegressor(loss=\"ls\",max_depth=20)\n",
    "\t# GBR10 = GradientBoostingRegressor(loss=\"lad\",max_depth=20)\n",
    "\t# GBR11 = GradientBoostingRegressor(loss=\"huber\",max_depth=20)\n",
    "\t# GBR12 = GradientBoostingRegressor(loss=\"quantile\",max_depth=20)\n",
    "\t# ETR1 = ExtraTreesRegressor(n_estimators=50)\n",
    "\tETR = ExtraTreesRegressor(n_estimators=50,criterion=\"mae\")\n",
    "\t# KNNR1 = KNeighborsRegressor(n_neighbors=5,p=1)\n",
    "\t# KNNR2 = KNeighborsRegressor(n_neighbors=20,p=1)\n",
    "\t# KNNR3 = KNeighborsRegressor(n_neighbors=5,p=2)\n",
    "\t# KNNR4 = KNeighborsRegressor(n_neighbors=20,p=2)\n",
    "\tKNNR = KNeighborsRegressor(n_neighbors=20,p=2)\n",
    "\tparams['kneighborsregressor__n_neighbors'] = [5,20]\n",
    "\t# MLPR = MLPRegressor()\n",
    "\t# SVR1 = SVR(kernel = \"linear\",C=100)\n",
    "\tSVRR = SVR(kernel = \"linear\",C=1)\n",
    "\t# SVR3 = SVR(kernel = \"linear\",C=0.01)\n",
    "\t# SVR4 = SVR()\n",
    "\tLR = LinearRegression()\n",
    "\t# HR1 = HuberRegressor(epsilon = 1.35,alpha =0.01)\n",
    "\tHR = HuberRegressor()\n",
    "\tparams['huberregressor__epsilon'] = [1.1,1.35,3]\n",
    "\t# HR2 = HuberRegressor(epsilon = 2,alpha =0.01)\n",
    "\t# HR3 = HuberRegressor(epsilon = 3,alpha =0.01)\n",
    "\t# HR4 = HuberRegressor(epsilon = 1.1,alpha =0.01)\n",
    "\t# HR5 = HuberRegressor(epsilon = 1.35,alpha =0.0001)\n",
    "\t# HR6 = HuberRegressor(epsilon = 2,alpha =0.0001)\n",
    "\t# HR7 = HuberRegressor(epsilon = 3,alpha =0.0001)\n",
    "\t# HR8 = HuberRegressor(epsilon = 1.1,alpha =0.0001)\n",
    "\tPAR1 = PassiveAggressiveRegressor(loss=\"epsilon_insensitive\")\n",
    "\tPAR2 = PassiveAggressiveRegressor(loss=\"squared_epsilon_insensitive\")\n",
    "\tBayesR = BayesianRidge()\n",
    "\tARDR = ARDRegression()\n",
    "\tLARSR = Lars()\n",
    "\tLassoLarsR = LassoLars()\n",
    "\tLassoLarsICR = LassoLarsIC()\n",
    "\t# MultiTaskLassoR = MultiTaskLasso()\n",
    "\tOrthogonalMatchingPursuitR = OrthogonalMatchingPursuit()\n",
    "\tRANSACRegressorR = RANSACRegressor()\n",
    "\tSGDR = SGDRegressor()\n",
    "\tTheilSenRegressorR = TheilSenRegressor()\n",
    "\tGaussianProcessRegressorR = GaussianProcessRegressor()\n",
    "\t# IsoR = IsotonicRegression()\n",
    "\t# RNR1 = RadiusNeighborsRegressor(radius=1.0)\n",
    "\t# RNR2 = RadiusNeighborsRegressor(radius=5)\n",
    "\t# RNR3 = RadiusNeighborsRegressor(radius=10)\n",
    "\tada_ridge = AdaBoostRegressor(base_estimator=Ridge())\n",
    "\t# ada_SVR = AdaBoostRegressor(base_estimator=SVR(kernel = \"linear\",C=1))\n",
    "\tada_KNN = AdaBoostRegressor(base_estimator=KNeighborsRegressor(n_neighbors=5))\n",
    "\tada_lasso = AdaBoostRegressor(base_estimator=Lasso())\n",
    "\tada_DT = AdaBoostRegressor()\n",
    "\tada_HR = AdaBoostRegressor(base_estimator=HuberRegressor(epsilon = 1.35,alpha =0.01))\n",
    "\tada_EN = AdaBoostRegressor(base_estimator=ElasticNet())\n",
    "\tada_LR = AdaBoostRegressor(base_estimator=LinearRegression())\n",
    "\txgbR = xgb.sklearn.XGBRegressor(n_estimators=1000)\n",
    "\t# my_reg_list = [ridge1,ridge2,ridge3,lasso1,lasso2,lasso3,lasso4,lasso5,lasso6,rf1,rf2,rf3,rf4,rf5,rf6,rf7,rf8,ENet1,ENet2,ENet3,ENet4,ENet5,ENet6,ENet7,ENet8,ENet9,GBR1,GBR2,GBR3,GBR4,GBR5,GBR6,GBR7,GBR8,GBR9,GBR10,GBR11,GBR12,ETR1,ETR2,KNNR1,KNNR2,KNNR3,KNNR4,MLPR,SVR1,SVR2,SVR3,SVR4,LR,HR1,HR2,HR3,HR4,HR5,HR6,HR7,HR8,PAR1,PAR2,BayesR,ARDR,LARSR,LassoLarsR,LassoLarsICR,MultiTaskLassoR,OrthogonalMatchingPursuitR,RANSACRegressorR,SGDR,TheilSenRegressorR,GaussianProcessRegressorR,IsoR,RNR1,RNR2,RNR3,ada_ridge,ada_SVR,ada_KNN,ada_lasso,ada_DT,ada_HR,ada_EN,ada_LR]\n",
    "\t# my_reg_list = [ridge1,ridge2,ridge3,lasso1,lasso2,lasso3,lasso4,lasso5,lasso6,rf4,rf8,ENet1,ENet2,ENet3,ENet4,ENet5,ENet6,ENet7,ENet8,ENet9,GBR1,GBR2,GBR3,GBR4,GBR5,GBR6,GBR7,GBR8,GBR9,GBR10,GBR11,GBR12,ETR1,ETR2,KNNR1,KNNR2,KNNR3,KNNR4,MLPR,SVR2,SVR3,SVR4,LR,HR1,HR2,HR3,HR4,HR5,HR6,HR7,HR8,PAR1,PAR2,BayesR,ARDR,LARSR,LassoLarsR,LassoLarsICR,OrthogonalMatchingPursuitR,RANSACRegressorR,SGDR,TheilSenRegressorR,GaussianProcessRegressorR,RNR1,RNR2,RNR3,ada_ridge,ada_KNN,ada_lasso,ada_DT,ada_HR,ada_EN,ada_LR]\n",
    "\tmy_reg_list = [ridge,lasso,rf,ENet,GBR,ETR,KNNR,SVRR,LR,HR,PAR1,PAR2,BayesR,ARDR,LARSR,LassoLarsR,LassoLarsICR,OrthogonalMatchingPursuitR,RANSACRegressorR,SGDR,TheilSenRegressorR,GaussianProcessRegressorR,ada_ridge,ada_KNN,ada_lasso,ada_DT,ada_HR,ada_EN,ada_LR,xgbR]\n",
    "\t# my_reg_list = [ridge1,xgbR,ridge3,lasso1]\n",
    "\tmix_CV_regressor = StackingCVRegressor(regressors=my_reg_list, meta_regressor=Lasso(),use_features_in_secondary = False,cv = 2)\t\n",
    "\t\n",
    "\tparams['meta-lasso__alpha'] = [0.01, 0.1 ,0.5, 1.0 ,5, 10.0]\n",
    "\t# params['xgbregressor__max_depth'] = [1,3,5,7,10,15,50]\n",
    "\t# params['xgbregressor__learning_rate'] = [0.1,0.01,1,10]\n",
    "\t# params['xgbregressor__gamma'] = [0.1,0.01,0.5,0.8]\n",
    "\t# params['xgbregressor__min_child_weight'] = [1,5,10,20]\n",
    "\t# params['xgbregressor__max_delta_step'] = [1,5,10,20]\n",
    "\t# params['xgbregressor__reg_alpha'] = [0.1,0.01,1,10]\n",
    "\t# params['xgbregressor__reg_lambda'] = [0.1,0.01,1,10]\n",
    "\t# params['meta-xgbregressor__max_depth'] = [1,3,5,7,10,15,50]\n",
    "\t# params['meta-xgbregressor__learning_rate'] = [0.1,0.01,1,10]\n",
    "\t# params['meta-xgbregressor__gamma'] = [0.1,0.01,0.5,0.8]\n",
    "\t# params['meta-xgbregressor__min_child_weight'] = [1,5,10,20]\n",
    "\t# params['meta-xgbregressor__max_delta_step'] = [1,5,10,20]\n",
    "\t# params['meta-xgbregressor__reg_alpha'] = [0.1,0.01,1,10]\n",
    "\t# params['meta-xgbregressor__reg_lambda'] = [0.1,0.01,1,10]\n",
    "\tgrid = RandomizedSearchCV(estimator=mix_CV_regressor, param_distributions=params, cv=2,n_iter=n_iter,refit=True,n_jobs=-1,scoring=\"neg_mean_absolute_error\",verbose=10)\t\t\t\t\t\t\n",
    "\tgrid.fit(X_train, y_train)\n",
    "\tprint('Best parameters: %s' % grid.best_params_)\n",
    "\tprint('CV best MAE: %.7f' % grid.best_score_)\n",
    "\tbest_clf = grid.best_estimator_\n",
    "\tpredict_value = best_clf.predict(X_test)\n",
    "\tMAE = mean_absolute_error(y_test,predict_value)\n",
    "\tMSE = mean_squared_error(y_test,predict_value)\n",
    "\tprint('testing MAE: %.7f' % MAE)\n",
    "\tprint('testing MSE: %.7f' % MSE)\n",
    "\treturn best_clf\n",
    "\t\n",
    "\n",
    "def scikitlearn_calc_auPRC(y_true, y_score):  \n",
    "\tprecision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "\treturn auc(recall, precision)\n",
    "\n",
    "def Balanced_ACC(y_true,y_pred,wrt=1):\n",
    "\ttp = 0.0\n",
    "\ttn = 0.0\n",
    "\tcorrect = 0.0\n",
    "\ty_true = list(y_true)\n",
    "\tif len(y_pred) != len(y_true):\n",
    "\t\tprint \"len(y_pred) != len(y_true)!\"\n",
    "\t\texit()\n",
    "\tfor i in range(len(y_pred)):\n",
    "\t\tif y_pred[i] == y_true[i]:\n",
    "\t\t\tif y_pred[i] == wrt:\n",
    "\t\t\t\ttp += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\ttn += 1\n",
    "\t\t\tcorrect += 1\n",
    "\ttotal = len(y_true)\n",
    "\tT = y_true.count(wrt)\n",
    "\tN = total - T\n",
    "\treturn (tp/T+tn/N)/2\n",
    "\n",
    "def mixer_clf(X,y,RANDOM_SEED = 42,n_iter=50):\n",
    "\t## n_iter is the number of parameter settings for RandomizedSearchCV\n",
    "\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\t# print y_test\n",
    "\t# print Balanced_ACC(y_test, y_test)\n",
    "\t# exit()\n",
    "\t# Initializing models\n",
    "\tclf1 = KNeighborsClassifier(n_neighbors=5)\n",
    "\tclf1_2 = KNeighborsClassifier(n_neighbors=5,weights=\"distance\")\n",
    "\tclf2 = RandomForestClassifier(random_state=RANDOM_SEED,n_estimators=50,criterion='gini',class_weight={0:.3, 1:.7})\n",
    "\tclf2_2 = RandomForestClassifier(random_state=RANDOM_SEED,n_estimators=50,criterion='entropy',class_weight={0:.3, 1:.7})\n",
    "\tclf12 = ExtraTreesClassifier(random_state=RANDOM_SEED,n_estimators=50,criterion='gini',class_weight={0:.3, 1:.7})\n",
    "\tclf12_2 = ExtraTreesClassifier(random_state=RANDOM_SEED,n_estimators=50,criterion='entropy',class_weight={0:.3, 1:.7})\n",
    "\tclf10 = GradientBoostingClassifier(learning_rate=0.05, subsample=0.8, max_depth=6, n_estimators=50)\n",
    "\tclf3 = GaussianNB()\n",
    "\tclf4 = SVC(probability=True,kernel=\"linear\",class_weight={0:.3, 1:.7})\n",
    "\tclf6 = GaussianProcessClassifier()\n",
    "\tclf7 = MLPClassifier()\n",
    "\tclf8 = AdaBoostClassifier(base_estimator=SVC(probability=True, kernel='linear',class_weight={0:.3, 1:.7}))\n",
    "\t# clf13 = AdaBoostClassifier(base_estimator=RidgeClassifierCV(),algorithm = 'SAMME')\n",
    "\tclf9 = QuadraticDiscriminantAnalysis()\n",
    "\t# clf13 = RidgeClassifierCV()\n",
    "\tclf14 = SGDClassifier(loss=\"log\")\n",
    "\tlr = LogisticRegression(class_weight={0:.3, 1:.7})\n",
    "\txgboost_clf = xgb.sklearn.XGBClassifier(n_estimators=1000,subsample=0.8,objective= 'binary:logistic')\n",
    "\n",
    "\tnp.random.seed(RANDOM_SEED)\n",
    "\tsclf = StackingCVClassifier(classifiers=[clf1, clf1_2,clf2,clf2_2,clf12,clf12_2,clf10,clf3,clf9,lr,clf4,xgboost_clf,clf7,clf8,clf14], \n",
    "\t\t\t\t\t\t\t\tmeta_classifier=xgboost_clf,use_probas = True,use_features_in_secondary = True)\n",
    "\tparams = {'logisticregression__C': [0.01,0.1,0.5,1,5,10],'svc__C':[0.01,0.5,10,100]}\n",
    "\tparams['xgbclassifier__learning_rate'] = [0.1,0.5,1]\n",
    "\tparams['xgbclassifier__max_depth'] = [5,10,20]\n",
    "\tparams['xgbclassifier__min_child_weight'] = [1,5,10]\n",
    "\tparams['xgbclassifier__gamma'] = [0,0.1,0.3,0.5,0.8]\n",
    "\tparams['xgbclassifier__colsample_bytree'] = [0.1,0.5,0.8]\n",
    "\tparams['xgbclassifier__max_delta_step'] = [0,0.1,0.4,0.8]\n",
    "\t# params['meta-svc__C'] = [0.01,0.1,0.5,1,5,10]\n",
    "\t# params['meta-logisticregression__C'] = [0.01,0.1,0.5,1,5,10]\n",
    "\t## tried many meta-classifiers, meta-logisticregression__C is the best\n",
    "\tparams['meta-xgbclassifier__learning_rate'] = [0.1,0.5,1]\n",
    "\tparams['meta-xgbclassifier__max_depth'] = [5,10,20]\n",
    "\tparams['meta-xgbclassifier__min_child_weight'] = [1,5,10]\n",
    "\tparams['meta-xgbclassifier__gamma'] = [0,0.1,0.3,0.5,0.8]\n",
    "\tparams['meta-xgbclassifier__colsample_bytree'] = [0.1,0.5,0.8]\n",
    "\t# grid = GridSearchCV(estimator=sclf, \n",
    "\t\t\t\t\t\t# param_grid=params, \n",
    "\t\t\t\t\t\t# cv=3,\n",
    "\t\t\t\t\t\t# refit=True,n_jobs=-1,scoring=\"average_precision\",verbose=10)\n",
    "\tgrid = RandomizedSearchCV(estimator=sclf, \n",
    "\t\t\t\t\t\tparam_distributions=params, \n",
    "\t\t\t\t\t\tcv=2,\n",
    "\t\t\t\t\t\tn_iter=n_iter,\n",
    "\t\t\t\t\t\trefit=True,n_jobs=-1,scoring=\"average_precision\",verbose=10)\t\t\t\t\t\n",
    "\n",
    "\tgrid.fit(X_train, y_train)\n",
    "\n",
    "\t# cv_keys = ('mean_test_score', 'std_test_score', 'params')\n",
    "\n",
    "\n",
    "\tprint('Best parameters: %s' % grid.best_params_)\n",
    "\tprint('CV best auPRC: %.2f' % grid.best_score_)\n",
    "\n",
    "\tbest_clf = grid.best_estimator_\n",
    "\n",
    "\tpredict_prob = best_clf.predict_proba(X_test)\n",
    "\tpredict_prob = np.array(map(lambda x:x[1],predict_prob)).ravel()\n",
    "\tauPRC = scikitlearn_calc_auPRC(y_test, predict_prob)\n",
    "\tauROC = roc_auc_score(y_test, predict_prob)\n",
    "\n",
    "\tY_pred = best_clf.predict(X_test)\n",
    "\t# print Y_pred\n",
    "\tMCC = matthews_corrcoef(y_test,Y_pred)\n",
    "\tf1 = f1_score(y_test,Y_pred)\n",
    "\tBACC = Balanced_ACC(y_test,Y_pred)\n",
    "\n",
    "\n",
    "\tprint('testing auROC: %.2f' % auROC)\n",
    "\tprint('testing auPRC: %.2f' % auPRC)\n",
    "\tprint('testing MCC: %.2f' % MCC)\n",
    "\tprint('testing f1: %.2f' % f1)\n",
    "\tprint('testing BACC: %.2f' % BACC)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\treturn best_clf\n",
    "\n",
    "\n",
    "def mixer_clf_train_test(X_train,X_test, y_train, y_test,RANDOM_SEED = 42,n_iter=50):\n",
    "\t## n_iter is the number of parameter settings for RandomizedSearchCV\n",
    "\n",
    "\t# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\t# print y_test\n",
    "\t# print Balanced_ACC(y_test, y_test)\n",
    "\t# exit()\n",
    "\t# Initializing models\n",
    "\tclf1 = KNeighborsClassifier(n_neighbors=5)\n",
    "\tclf1_2 = KNeighborsClassifier(n_neighbors=5,weights=\"distance\")\n",
    "\tclf2 = RandomForestClassifier(random_state=RANDOM_SEED,n_estimators=50,criterion='gini',class_weight={0:.3, 1:.7})\n",
    "\tclf2_2 = RandomForestClassifier(random_state=RANDOM_SEED,n_estimators=50,criterion='entropy',class_weight={0:.3, 1:.7})\n",
    "\tclf12 = ExtraTreesClassifier(random_state=RANDOM_SEED,n_estimators=50,criterion='gini',class_weight={0:.3, 1:.7})\n",
    "\tclf12_2 = ExtraTreesClassifier(random_state=RANDOM_SEED,n_estimators=50,criterion='entropy',class_weight={0:.3, 1:.7})\n",
    "\tclf10 = GradientBoostingClassifier(learning_rate=0.05, subsample=0.8, max_depth=6, n_estimators=50)\n",
    "\tclf3 = GaussianNB()\n",
    "\tclf4 = SVC(probability=True,kernel=\"linear\",class_weight={0:.3, 1:.7})\n",
    "\tclf6 = GaussianProcessClassifier()\n",
    "\tclf7 = MLPClassifier()\n",
    "\tclf8 = AdaBoostClassifier(base_estimator=SVC(probability=True, kernel='linear',class_weight={0:.3, 1:.7}))\n",
    "\t# clf13 = AdaBoostClassifier(base_estimator=RidgeClassifierCV(),algorithm = 'SAMME')\n",
    "\tclf9 = QuadraticDiscriminantAnalysis()\n",
    "\t# clf13 = RidgeClassifierCV()\n",
    "\tclf14 = SGDClassifier(loss=\"log\")\n",
    "\tlr = LogisticRegression(class_weight={0:.3, 1:.7})\n",
    "\txgboost_clf = xgb.sklearn.XGBClassifier(n_estimators=1000,subsample=0.8,objective= 'binary:logistic')\n",
    "\n",
    "\tnp.random.seed(RANDOM_SEED)\n",
    "\tsclf = StackingCVClassifier(classifiers=[clf1, clf1_2,clf2,clf2_2,clf12,clf12_2,clf10,clf3,clf9,lr,clf4,xgboost_clf,clf7,clf8,clf14], \n",
    "\t\t\t\t\t\t\t\tmeta_classifier=lr,use_probas = True,use_features_in_secondary = True)\n",
    "\tparams = {'logisticregression__C': [0.01,0.1,0.5,1,5,10],'svc__C':[0.01,0.5,10,100]}\n",
    "\tparams['xgbclassifier__learning_rate'] = [0.1,0.5,1]\n",
    "\tparams['xgbclassifier__max_depth'] = [5,10,20]\n",
    "\tparams['xgbclassifier__min_child_weight'] = [1,5,10]\n",
    "\tparams['xgbclassifier__gamma'] = [0,0.1,0.3,0.5,0.8]\n",
    "\tparams['xgbclassifier__colsample_bytree'] = [0.1,0.5,0.8]\n",
    "\tparams['xgbclassifier__max_delta_step'] = [0,0.1,0.4,0.8]\n",
    "\t# params['meta-svc__C'] = [0.01,0.1,0.5,1,5,10]\n",
    "\tparams['meta-logisticregression__C'] = [0.01,0.1,0.5,1,5,10]\n",
    "\t## tried many meta-classifiers, meta-logisticregression__C is the best\n",
    "\t# params['meta-xgbclassifier__learning_rate'] = [0.1,0.5,1]\n",
    "\t# params['meta-xgbclassifier__max_depth'] = [5,10,20]\n",
    "\t# params['meta-xgbclassifier__min_child_weight'] = [1,5,10]\n",
    "\t# params['meta-xgbclassifier__gamma'] = [0,0.1,0.3,0.5,0.8]\n",
    "\t# params['meta-xgbclassifier__colsample_bytree'] = [0.1,0.5,0.8]\n",
    "\t# grid = GridSearchCV(estimator=sclf, \n",
    "\t\t\t\t\t\t# param_grid=params, \n",
    "\t\t\t\t\t\t# cv=3,\n",
    "\t\t\t\t\t\t# refit=True,n_jobs=-1,scoring=\"average_precision\",verbose=10)\n",
    "\tgrid = RandomizedSearchCV(estimator=sclf, \n",
    "\t\t\t\t\t\tparam_distributions=params, \n",
    "\t\t\t\t\t\tcv=2,\n",
    "\t\t\t\t\t\tn_iter=n_iter,\n",
    "\t\t\t\t\t\trefit=True,n_jobs=-1,scoring=\"average_precision\",verbose=10)\t\t\t\t\t\n",
    "\n",
    "\tgrid.fit(X_train, y_train)\n",
    "\n",
    "\t# cv_keys = ('mean_test_score', 'std_test_score', 'params')\n",
    "\n",
    "\n",
    "\tprint('Best parameters: %s' % grid.best_params_)\n",
    "\tprint('CV best avg precision: %.2f' % grid.best_score_)\n",
    "\n",
    "\tbest_clf = grid.best_estimator_\n",
    "\n",
    "\tpredict_prob = best_clf.predict_proba(X_test)\n",
    "\tpredict_prob = np.array(map(lambda x:x[1],predict_prob)).ravel()\n",
    "\tauPRC = scikitlearn_calc_auPRC(y_test, predict_prob)\n",
    "\tauROC = roc_auc_score(y_test, predict_prob)\n",
    "\n",
    "\tY_pred = best_clf.predict(X_test)\n",
    "\t# print Y_pred\n",
    "\tMCC = matthews_corrcoef(y_test,Y_pred)\n",
    "\tf1 = f1_score(y_test,Y_pred)\n",
    "\tBACC = Balanced_ACC(y_test,Y_pred)\n",
    "\n",
    "\n",
    "\tprint('testing auROC: %.2f' % auROC)\n",
    "\tprint('testing auPRC: %.2f' % auPRC)\n",
    "\tprint('testing MCC: %.2f' % MCC)\n",
    "\tprint('testing f1: %.2f' % f1)\n",
    "\tprint('testing BACC: %.2f' % BACC)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\treturn best_clf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for XGBoost ...\n",
      "Shape train: (90275, 58)\n",
      "Shape test: (2985217, 58)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix\n",
    "directory = \"./\"\n",
    "properties = pd.read_csv(\"/home/working/my_second_drive/projects/liyc_own/Zillow/\"+'properties_2016.csv')\n",
    "train = pd.read_csv(directory+\"train_2016_v2.csv\")\n",
    "print( \"\\nProcessing data for XGBoost ...\")\n",
    "for c in properties.columns:\n",
    "    properties[c]=properties[c].fillna(-1)\n",
    "    if properties[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(properties[c].values))\n",
    "        properties[c] = lbl.transform(list(properties[c].values))\n",
    "\n",
    "train_df = train.merge(properties, how='left', on='parcelid')\n",
    "\n",
    "train_df[\"transactiondate\"] = pd.to_datetime(train_df[\"transactiondate\"])\n",
    "train_df[\"Month\"] = train_df[\"transactiondate\"].dt.month\n",
    "\n",
    "x_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n",
    "x_test = properties.drop(['parcelid'], axis=1)\n",
    "\n",
    "x_test[\"transactiondate\"] = '2016-07-01'\n",
    "x_test[\"transactiondate\"] = pd.to_datetime(x_test[\"transactiondate\"])\n",
    "x_test[\"Month\"] = x_test[\"transactiondate\"].dt.month #should use the most common training date 2016-10-01\n",
    "x_test = x_test.drop(['transactiondate'], axis=1)\n",
    "\n",
    "# shape        \n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "# drop out ouliers\n",
    "train_df=train_df[ train_df.logerror > -0.4 ]\n",
    "train_df=train_df[ train_df.logerror < 0.419 ]\n",
    "x_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n",
    "y_train = train_df[\"logerror\"].values.astype(np.float32)\n",
    "x_train = x_train.values.astype(np.float32, copy=False)\n",
    "x_test = x_test.values.astype(np.float32, copy=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00, ...,\n",
       "         -1.00000000e+00,   6.03710669e+13,   1.00000000e+00],\n",
       "       [ -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00, ...,\n",
       "         -1.00000000e+00,  -1.00000000e+00,   1.00000000e+00],\n",
       "       [  1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00, ...,\n",
       "         -1.00000000e+00,   6.03746363e+13,   1.00000000e+00],\n",
       "       ..., \n",
       "       [ -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00, ...,\n",
       "         -1.00000000e+00,   6.03730089e+13,   1.20000000e+01],\n",
       "       [ -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00, ...,\n",
       "          1.40000000e+01,   6.03743259e+13,   1.20000000e+01],\n",
       "       [ -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00, ...,\n",
       "         -1.00000000e+00,   6.03760120e+13,   1.20000000e+01]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "[CV] ridge__alpha=10, lasso__alpha=1, meta-lasso__alpha=10.0 .........\n",
      "[CV] ridge__alpha=10, lasso__alpha=1, meta-lasso__alpha=10.0 .........\n",
      "[CV] ridge__alpha=5, lasso__alpha=10, meta-lasso__alpha=0.1 ..........\n",
      "[CV] ridge__alpha=5, lasso__alpha=10, meta-lasso__alpha=0.1 ..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 7.1712605472e-12\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 1.04259705036e-10\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 6.65447558168e-11\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 4.06261135844e-12\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 2.49921527917e-10\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 1.02911373867e-10\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 6.87235199304e-11\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 1.86437448813e-10\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 2.53157321495e-11\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 1.12181160195e-10\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 1.90577588433e-12\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 1.88445758997e-10\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ridge__alpha=5, lasso__alpha=10, meta-lasso__alpha=0.1, score=-0.0536348380935, total=71.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 71.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ridge__alpha=5, lasso__alpha=10, meta-lasso__alpha=0.1, score=-0.0526281792432, total=72.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed: 72.7min remaining: 72.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ridge__alpha=10, lasso__alpha=1, meta-lasso__alpha=10.0, score=-0.0536348380935, total=90.1min\n",
      "[CV]  ridge__alpha=10, lasso__alpha=1, meta-lasso__alpha=10.0, score=-0.0526281792432, total=90.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 90.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 90.9min finished\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 2.3032038516e-12\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 7.55953077913e-11\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 5.04556500747e-11\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "model = mixer_reg(x_train,y_train,n_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
